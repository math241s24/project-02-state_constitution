---
title: "State Constitution Analysis"
subtitle: "Project 2"
author: "Cassie Minicucci, Katie Riley, and Basil-Anne Stackpole"
title-slide-attributes:
  data-slide-number: none
format: revealjs
editor: visual
execute:
  echo: false
---

```{r}
#| label: load-packages
#| include: false

library(tidyverse)
library(tmap)
library(tigris)
library(tidyr)
library(tidytext)
library(rvest)
library(dplyr)
```

```{r}
#| label: setup
#| include: false

# For better figure resolution
knitr::opts_chunk$set(
  fig.retina = 3, 
  dpi = 300, 
  fig.width = 6, 
  fig.asp = 0.618, 
  out.width = "70%"
  )
```

```{r}
#| label: load-data
#| include: false
```

## Does the time a state came into being affect the keywords used in its Constitution?

We scraped text from the constitutions of the original 13 colonies and analyzed each for specific keywords.

-   Keywords: freedom, liberty, welfare, consent, prohibit, equality, and rights

## Does the time a state came into being affect the keywords used in its Constitution?

Then, we created an interactive map using a time variable to be able to see when each state was ratified.

Goal: to understand if the time a state was ratified has a large effect on the language used in the constitution.

## Game plan {.smaller}

-   Scrape text from each constitution and create separate dataframes for each
-   Find frequencies of chosen keywords in each constitution 
-   Also analyze the top 20 words appearing most frequently in each constitution (minus common stop words)
-   Create histograms of the most common words
-   Link dataframes together into one dataframe to create maps
-   Created character variable for each word with what state it was from, then used the tigris package with state geometry to create a map (keywords!)
-   Create interactive map!

## Ratification years {.smaller}

| state  | year   |
|--------|--------|
| Delaware  | 1787 |
| Georgia   | 1777  |
| Connecticut | 1776 |
| Maryland  | 1776  |
| Massachusetts   | 1780  |
| New Hampshire | 1776   |
| New Jersey  | 1776   |
| New York   | 1777   |
| North Carolina | 1776   |
| Pennsylvania  | 1776   |
| Rhode Island   | 1843   |
| South Carolina | 1778   |
| Virginia | 1776 |


## Word frequencies: Virginia {.smaller}
Ratification year: 1776

::: {.panel-tabset}

### Most common words
```{r, echo=FALSE}
va_url <- "https://www.wordservice.org/State%20Constitutions/usa1025.htm"
va_html <- read_html(va_url)

va_text <- html_text(va_html)

va_con <- data.frame(word = unlist(strsplit(va_text, "\\s+"))) %>%
  filter(word != "")  %>%
  mutate(word = str_replace_all(word, "[[:punct:][:space:]]+", "")) 

rows_to_exclude_va <- c(1:201, 3680:3689)

va_con_tidy <- va_con %>%
  mutate(word = tolower(word)) %>%
  filter(!(row_number() %in% rows_to_exclude_va))

va_con_join <- va_con_tidy %>%
  mutate(state = rep("Virgina"))
```

```{r, echo=FALSE}
va_word_count <- va_con_tidy %>%
  filter(word %in%
  c("freedom", "liberty", "welfare", "consent", "prohibit", "equality", "rights")) %>%
  count(word)

data("stop_words")
va_con_tidy_nostop <- va_con_tidy %>%
 anti_join(stop_words) 
va_con_tidy_nostop %>%
  count(word, sort = TRUE) %>%
    slice_max(n = 20, order_by = n) %>%
  filter(!word%in%c("(--", " "))

```

### Histogram
```{r, echo=FALSE}
va_con_tidy_nostop %>%
  count(word, sort = TRUE) %>%
  top_n(20) %>%
  ggplot(aes(y = fct_reorder(word, n), x = n, fill = n)) +
  geom_col() +
  guides(fill = FALSE) +
  labs(y = "Most Common Words")
```

:::

## Word frequencies: Pennsylvania {.smaller}
Ratification year: 1776

::: {.panel-tabset}

### Most common words
```{r, echo=FALSE}
pa_url <- "https://avalon.law.yale.edu/18th_century/pa08.asp"
pa_html <- read_html(pa_url)

pa_text <- html_text(pa_html)

pa_con <- data.frame(word = unlist(strsplit(pa_text, "\\s+"))) %>%
  filter(word != "")  %>%
  mutate(word = str_replace_all(word, "[[:punct:][:space:]]+", "")) 

rows_to_exclude <- c(1:57, 6196:6533)

pa_con_tidy <- pa_con %>%
  mutate(word = tolower(word)) %>%
  filter(!(row_number() %in% rows_to_exclude))

pa_con_join <- pa_con_tidy %>%
  mutate(state = rep("Pennsylvania"))
```

```{r, echo=FALSE}
library(tidyverse)
library(tidytext)
pa_word_count <- pa_con_tidy %>%
  filter(word %in%
  c("freedom", "liberty", "welfare", "consent", "prohibit", "equality", "rights")) %>%
  count(word)

data("stop_words")
pa_con_tidy_nostop <- pa_con_tidy %>%
 anti_join(stop_words) 
pa_con_tidy_nostop %>%
  count(word, sort = TRUE) %>%
    slice_max(n = 20, order_by = n) %>%
  filter(!word%in%c("(--", " "))
```

### Histogram
```{r, echo=FALSE}
pa_con_tidy_nostop %>%
  count(word, sort = TRUE) %>%
  top_n(20) %>%
  ggplot(aes(y = fct_reorder(word, n), x = n, fill = n)) +
  geom_col() +
  guides(fill = FALSE)+
  labs(y = "Most Common Words")
```

:::

## Word frequencies: Maryland {.smaller}
Ratification year: 1776

::: {.panel-tabset}

### Most common words
```{r, echo=FALSE}
library(rvest)
library(dplyr)

md_url <- "https://avalon.law.yale.edu/17th_century/ma02.asp"
md_html <- read_html(md_url)

md_text <- html_text(md_html)

md_con <- data.frame(word = unlist(strsplit(md_text, "\\s+"))) %>%
  filter(word != "")  %>%
  mutate(word = str_replace_all(word, "[[:punct:][:space:]]+", "")) 

rows_to_exclude_md <- c(1:94, 2623:2669, 8854:9084)

md_con_tidy <- md_con %>%
  mutate(word = tolower(word)) %>%
  filter(!(row_number() %in% rows_to_exclude_md))

md_con_join <- md_con_tidy %>%
  mutate(state = rep("Maryland"))
```

```{r, echo=FALSE}
library(tidyverse)
library(tidytext)
md_word_count <- md_con_tidy %>%
  filter(word %in%
  c("freedom", "liberty", "welfare", "consent", "prohibit", "equality", "rights")) %>%
  count(word)

data("stop_words")
md_con_tidy_nostop <- md_con_tidy %>%
 anti_join(stop_words) 
md_con_tidy_nostop %>%
  count(word, sort = TRUE) %>%
    slice_max(n = 20, order_by = n) 
```

### Histogram
```{r, echo=FALSE}
md_con_tidy_nostop %>%
  count(word, sort = TRUE) %>%
  top_n(20) %>%
  ggplot(aes(y = fct_reorder(word, n), x = n, fill = n)) +
  geom_col() +
  guides(fill = FALSE)+
  labs(y = "Most Common Words")
```

:::

## Word frequencies: New Jersey {.smaller}
Ratification year: 1776

::: {.panel-tabset}

### Most common words
```{r, echo=FALSE}
library(rvest)
library(dplyr)

nj_url <- "https://avalon.law.yale.edu/18th_century/nj15.asp"
nj_html <- read_html(nj_url)

nj_text <- html_text(nj_html)

nj_con <- data.frame(word = unlist(strsplit(nj_text, "\\s+"))) %>%
  filter(word != "")  %>%
  mutate(word = str_replace_all(word, "[[:punct:][:space:]]+", "")) 

rows_to_exclude_nj <- c(1:55, 2524:2889)

nj_con_tidy <- nj_con %>%
  mutate(word = tolower(word)) %>%
  filter(!(row_number() %in% rows_to_exclude_nj))
```

```{r, echo=FALSE}
library(tidyverse)
library(tidytext)
nj_word_count <- nj_con_tidy %>%
  filter(word %in%
  c("freedom", "liberty", "welfare", "consent", "prohibit", "equality", "rights")) %>%
  count(word)

data("stop_words")
nj_con_tidy_nostop <- nj_con_tidy %>%
 anti_join(stop_words) 
nj_con_tidy_nostop %>%
  count(word, sort = TRUE) %>%
    slice_max(n = 20, order_by = n) %>%
  filter(!word%in%c("(art.", " "))
```

### Histogram
```{r, echo=FALSE}
nj_con_tidy_nostop %>%
  count(word, sort = TRUE) %>%
  top_n(20) %>%
  ggplot(aes(y = fct_reorder(word, n), x = n, fill = n)) +
  geom_col() +
  guides(fill = FALSE)+
  labs(y = "Most Common Words")
```

:::

## Word frequencies: New Hampshire {.smaller}
Ratification year: 1776

::: {.panel-tabset}

### Most common words
```{r, echo=FALSE}
library(rvest)
library(dplyr)

nh_url <- "https://avalon.law.yale.edu/18th_century/nh09.asp"
nh_html <- read_html(nh_url)

nh_text <- html_text(nh_html)

nh_con <- data.frame(word = unlist(strsplit(nh_text, "\\s+"))) %>%
  filter(word != "")  %>%
  mutate(word = str_replace_all(word, "[[:punct:][:space:]]+", "")) 

rows_to_exclude_nh <- c(1:63, 994:1242)

nh_con_tidy <- nh_con %>%
  mutate(word = tolower(word)) %>%
  filter(!(row_number() %in% rows_to_exclude_nh))

nh_con_join <- nh_con_tidy %>%
  mutate(state = rep("New Hampshire"))
```

```{r, echo=FALSE}
library(tidyverse)
library(tidytext)
nh_word_count <- nh_con_tidy %>%
  filter(word %in%
  c("freedom", "liberty", "welfare", "consent", "prohibit", "equality", "rights")) %>%
  count(word)

data("stop_words")
nh_con_tidy_nostop <- nh_con_tidy %>%
 anti_join(stop_words) 
nh_con_tidy_nostop %>%
  count(word, sort = TRUE) %>%
    slice_max(n = 20, order_by = n) %>%
  filter(!word%in%c("(art.", " "))
```

### Histogram
```{r, echo=FALSE}
nh_con_tidy_nostop %>%
  count(word, sort = TRUE) %>%
  top_n(20) %>%
  ggplot(aes(y = fct_reorder(word, n), x = n, fill = n)) +
  geom_col() +
  guides(fill = FALSE)+
  labs(y = "Most Common Words")
```

:::

## Word frequencies: Connecticut {.smaller}
Ratification year: 1776

::: {.panel-tabset}

### Most common words
```{r, echo=FALSE}
library(rvest)
library(dplyr)

ct_url <- "https://www.cga.ct.gov/asp/Content/constitutions/1818Constitution.htm"
ct_html <- read_html(ct_url)

ct_text <- html_text(ct_html)

ct_con <- data.frame(word = unlist(strsplit(ct_text, "\\s+"))) %>%
  filter(word != "")  %>%
  mutate(word = str_replace_all(word, "[[:punct:][:space:]]+", "")) %>%
  filter(!word=="")

rows_to_exclude_ct <- c(1:7, 223:298, 6012:12230)

ct_con_tidy <- ct_con %>%
  mutate(word = tolower(word)) %>%
  filter(!(row_number() %in% rows_to_exclude_ct))

ct_con_join <- ct_con_tidy %>%
  mutate(state = rep("Connecticut"))

```

```{r, echo=FALSE}
library(tidyverse)
library(tidytext)
ct_word_count <- ct_con_tidy %>%
  filter(word %in%
  c("freedom", "liberty", "welfare", "consent", "prohibit", "equality", "rights")) %>%
  count(word)

data("stop_words")
ct_con_tidy_nostop <- ct_con_tidy %>%
 anti_join(stop_words) 
ct_con_tidy_nostop %>%
  count(word, sort = TRUE) %>%
    slice_max(n = 20, order_by = n) 

```

### Histogram
```{r, echo=FALSE}
ct_con_tidy_nostop %>%
  count(word, sort = TRUE) %>%
  top_n(20) %>%
  ggplot(aes(y = fct_reorder(word, n), x = n, fill = n)) +
  geom_col() +
  guides(fill = FALSE)+
  labs(y = "Most Common Words")
```

:::

## Word frequencies: North Carolina {.smaller}
Ratification year: 1776

::: {.panel-tabset}

### Most common words
```{r, echo=FALSE}

library(rvest)
library(dplyr)

nc_url <- "https://avalon.law.yale.edu/18th_century/nc07.asp"
nc_html <- read_html(nc_url)

nc_text <- html_text(nc_html)

nc_con <- data.frame(word = unlist(strsplit(nc_text, "\\s+"))) %>%
  filter(word != "")  %>%
  mutate(word = str_replace_all(word, "[[:punct:][:space:]]+", "")) 

rows_to_exclude <- c(1:58, 3991:4231)

nc_con_tidy <- nc_con %>%
  mutate(word = tolower(word)) %>%
  filter(!(row_number() %in% rows_to_exclude))

nc_con_join <- nc_con_tidy %>%
  mutate(state = rep("North Carolina"))
```

```{r, echo=FALSE}
library(tidyverse)
library(tidytext)
nc_word_count <- nc_con_tidy %>%
  filter(word %in%
  c("freedom", "liberty", "welfare", "consent", "prohibit", "equality", "rights")) %>%
  count(word)

data("stop_words")
nc_con_tidy_nostop <- nc_con_tidy %>%
 anti_join(stop_words) 
nc_con_tidy_nostop %>%
  count(word, sort = TRUE) %>%
    slice_max(n = 20, order_by = n) %>%
  filter(!word%in%c("(--", " "))
```

### Histogram
```{r, echo=FALSE}
nc_con_tidy_nostop %>%
  count(word, sort = TRUE) %>%
  top_n(20) %>%
  ggplot(aes(y = fct_reorder(word, n), x = n, fill = n)) +
  geom_col() +
  guides(fill = FALSE)+
  labs(y = "Most Common Words")
```

:::

## Word frequencies: New York {.smaller}
Ratification year: 1777

::: {.panel-tabset}

### Most common words
```{r, echo=FALSE}
library(rvest)
library(dplyr)

ny_url <- "https://avalon.law.yale.edu/18th_century/ny01.asp"
ny_html <- read_html(ny_url)

ny_text <- html_text(ny_html)

ny_con <- data.frame(word = unlist(strsplit(ny_text, "\\s+"))) %>%
  filter(word != "")  %>%
  mutate(word = str_replace_all(word, "[[:punct:][:space:]]+", "")) 

rows_to_exclude <- c(1:68, 7813:8781)

ny_con_tidy <- ny_con %>%
  mutate(word = tolower(word)) %>%
  filter(!(row_number() %in% rows_to_exclude))

ny_con_join <- ny_con_tidy %>%
  mutate(state = rep("New York"))
```

```{r, echo=FALSE}
library(tidyverse)
library(tidytext)

ny_word_count <- ny_con_tidy %>%
  filter(word %in%
  c("freedom", "liberty", "welfare", "consent", "prohibit", "equality", "rights")) %>%
  count(word)

data("stop_words")
ny_con_tidy_nostop <- ny_con_tidy %>%
 anti_join(stop_words) 
ny_con_tidy_nostop %>%
  count(word, sort = TRUE) %>%
    slice_max(n = 20, order_by = n)
```

### Histogram
```{r, echo=FALSE}
ny_con_tidy_nostop %>%
  count(word, sort = TRUE) %>%
  top_n(20) %>%
  ggplot(aes(y = fct_reorder(word, n), x = n, fill = n)) +
  geom_col() +
  guides(fill = FALSE)+
  labs(y = "Most Common Words")
```

:::

## Word frequencies: Georgia {.smaller}
Ratification year: 1777

::: {.panel-tabset}

### Most common words
```{r, echo=FALSE}
library(rvest)
library(dplyr)

ga_url <- "https://avalon.law.yale.edu/18th_century/ga02.asp"
ga_html <- read_html(ga_url)

ga_text <- html_text(ga_html)

ga_con <- data.frame(word = unlist(strsplit(ga_text, "\\s+"))) %>%
  filter(word != "")  %>%
  mutate(word = str_replace_all(word, "[[:punct:][:space:]]+", "")) 

rows_to_exclude_ga <- c(1:56, 4422:4652)

ga_con_tidy <- ga_con %>%
  mutate(word = tolower(word)) %>%
  filter(!(row_number() %in% rows_to_exclude_ga))

ga_con_join <- ga_con_tidy %>%
  mutate(state = rep("Georgia"))

```

```{r, echo=FALSE}
library(tidyverse)
library(tidytext)
ga_word_count <- ga_con_tidy %>%
  filter(word %in%
  c("freedom", "liberty", "welfare", "consent", "prohibit", "equality", "rights")) %>%
  count(word)

data("stop_words")
ga_con_tidy_nostop <- ga_con_tidy %>%
 anti_join(stop_words) 
ga_con_tidy_nostop %>%
  count(word, sort = TRUE) %>%
    slice_max(n = 20, order_by = n) 
```

### Histogram
```{r}
ga_con_tidy_nostop %>%
  count(word, sort = TRUE) %>%
  top_n(20) %>%
  ggplot(aes(y = fct_reorder(word, n), x = n, fill = n)) +
  geom_col() +
  guides(fill = FALSE)+
  labs(y = "Most Common Words")
```

:::

## Word frequencies: South Carolina {.smaller}
Ratification year: 1778

::: {.panel-tabset}

### Most common words
```{r, echo=FALSE}
library(rvest)
library(dplyr)

sc_url <- "https://avalon.law.yale.edu/18th_century/sc02.asp"
sc_html <- read_html(sc_url)

sc_text <- html_text(sc_html)

sc_con <- data.frame(word = unlist(strsplit(sc_text, "\\s+"))) %>%
  filter(word != "")  %>%
  mutate(word = str_replace_all(word, "[[:punct:][:space:]]+", "")) 

rows_to_exclude <- c(1:58, 5639:5880)

sc_con_tidy <- sc_con %>%
  mutate(word = tolower(word)) %>%
  filter(!(row_number() %in% rows_to_exclude))

sc_con_join <- sc_con_tidy %>%
  mutate(state = rep("South Carolina"))
```

```{r, echo=FALSE}
library(tidyverse)
library(tidytext)
sc_word_count <- sc_con_tidy %>%
  filter(word %in%
  c("freedom", "liberty", "welfare", "consent", "prohibit", "equality", "rights")) %>%
  count(word)

data("stop_words")
sc_con_tidy_nostop <- sc_con_tidy %>%
 anti_join(stop_words) 
sc_con_tidy_nostop %>%
  count(word, sort = TRUE) %>%
    slice_max(n = 20, order_by = n) %>%
  filter(!word%in%c("(--", " "))
```

### Histogram
```{r, echo=FALSE}
sc_con_tidy_nostop %>%
  count(word, sort = TRUE) %>%
  top_n(20) %>%
  ggplot(aes(y = fct_reorder(word, n), x = n, fill = n)) +
  geom_col() +
  guides(fill = FALSE)+
  labs(y = "Most Common Words")
```

:::

## Word frequencies: Massachusetts {.smaller}
Ratification year: 1780

::: {.panel-tabset}

### Most common words
```{r, echo=FALSE}
ma_url <- "http://www.nhinet.org/ccs/docs/ma-1780.htm"
ma_html <- read_html(ma_url)

ma_text <- html_text(ma_html)

ma_con <- data.frame(word = unlist(strsplit(ma_text, "\\s+"))) %>%
  filter(word != "")  %>%
  mutate(word = str_replace_all(word, "[[:punct:][:space:]]+", "")) 

rows_to_exclude_ma <- c(11433:11445)

ma_con_tidy <- ma_con %>%
  mutate(word = tolower(word)) %>%
  mutate(word=gsub("[[:space:]]", "", word)) %>%
  filter(!(row_number() %in% rows_to_exclude_ma))

ma_con_join <- ma_con_tidy %>%
  mutate(state = rep("Massachusetts"))

```

```{r, echo=FALSE}
library(tidyverse)
library(tidytext)
ma_word_count <- ma_con_tidy %>%
  filter(word %in%
  c("freedom", "liberty", "welfare", "consent", "prohibit", "equality", "rights")) %>%
  count(word)

data("stop_words")
ma_con_tidy_nostop <- ma_con_tidy %>%
 anti_join(stop_words) 
ma_con_tidy_nostop %>%
  count(word, sort = TRUE) %>%
    slice_max(n = 20, order_by = n) %>%
  filter(!word%in%c("(art.", ""))

```

### Histogram
```{r, echo=FALSE}
ma_con_tidy_nostop %>%
  count(word, sort = TRUE) %>%
  top_n(20) %>%
  ggplot(aes(y = fct_reorder(word, n), x = n, fill = n)) +
  geom_col() +
  guides(fill = FALSE)+
  labs(y = "Most Common Words")
```

:::


## Word frequencies: Delaware {.smaller}
Ratification year: 1787

::: {.panel-tabset}

### Most common words
```{r, echo=FALSE}
library(rvest)
library(dplyr)

de_url <- "https://avalon.law.yale.edu/18th_century/de02.asp"
de_html <- read_html(de_url)

de_text <- html_text(de_html)

de_con <- data.frame(word = unlist(strsplit(de_text, "\\s+"))) %>%
  filter(word != "")  %>%
  mutate(word = str_replace_all(word, "[[:punct:][:space:]]+", "")) 

rows_to_exclude <- c(1:54, 3721:3796)

de_con_tidy <- de_con %>%
  mutate(word = tolower(word)) %>%
  filter(!(row_number() %in% rows_to_exclude))

de_con_join <- de_con_tidy %>%
  mutate(state = rep("Delaware"))

```

```{r, echo=FALSE}
library(tidyverse)
library(tidytext)
de_word_count <- de_con_tidy %>%
  filter(word %in%
  c("freedom", "liberty", "welfare", "consent", "prohibit", "equality", "rights")) %>%
  count(word)

data("stop_words")
de_con_tidy_nostop <- de_con_tidy %>%
 anti_join(stop_words) 
de_con_tidy_nostop %>%
  count(word, sort = TRUE) %>%
    slice_max(n = 20, order_by = n) 
```

### Histogram
```{r, echo=FALSE}
de_con_tidy_nostop %>%
  count(word, sort = TRUE) %>%
  top_n(20) %>%
  ggplot(aes(y = fct_reorder(word, n), x = n, fill = n)) +
  geom_col() +
  guides(fill = FALSE)+
  labs(y = "Most Common Words")
```

:::

## Word frequencies: Rhode Island {.smaller}
Ratification year: 1843

::: {.panel-tabset}

### Most common words
```{r, echo=FALSE}
ri_url <- "https://www.wordservice.org/State%20Constitutions/usa1031.htm"
ri_html <- read_html(ri_url)

ri_text <- html_text(ri_html)

ri_con <- data.frame(word = unlist(strsplit(ri_text, "\\s+"))) %>%
  filter(word != "")  %>%
  mutate(word = str_replace_all(word, "[[:punct:][:space:]]+", ""))  %>%
  filter(!word=="")



rows_to_exclude_ri <- c(1:200, 7322:7331)

ri_con_tidy <- ri_con %>%
  mutate(word = tolower(word)) %>%
  filter(!(row_number() %in% rows_to_exclude_ri))

ri_con_join <- ri_con_tidy %>%
  mutate(state = rep("Rhode Island"))
```

```{r, echo=FALSE}
library(tidyverse)
library(tidytext)
ri_word_count <- ri_con_tidy %>%
  filter(word %in%
  c("freedom", "liberty", "welfare", "consent", "prohibit", "equality", "rights")) %>%
  count(word)

data("stop_words")
ri_con_tidy_nostop <- ri_con_tidy %>%
 anti_join(stop_words) 
ri_con_tidy_nostop %>%
  count(word, sort = TRUE) %>%
    slice_max(n = 20, order_by = n) %>%
  filter(!word%in%c("(--", " "))

```

### Histogram
```{r, echo=FALSE}
ri_con_tidy_nostop %>%
  count(word, sort = TRUE) %>%
  top_n(20) %>%
  ggplot(aes(y = fct_reorder(word, n), x = n, fill = n)) +
  geom_col() +
  guides(fill = FALSE)+
  labs(y = "Most Common Words")
```

:::


## Keyword Maps {.smaller}

::: {.panel-tabset}

### Freedom
```{r, echo=FALSE}
ct_word_count <- ct_word_count %>%
  mutate(state = rep("Connecticut"))

de_word_count <- de_word_count %>%
  mutate(state = rep("Delaware"))

ga_word_count <- ga_word_count %>%
  mutate(state = rep("Georgia"))

ma_word_count <- ma_word_count %>%
  mutate(state = rep("Massachusetts"))

md_word_count <- md_word_count %>%
  mutate(state = rep("Maryland"))

nc_word_count <- nc_word_count %>%
  mutate(state = rep("North Carolina"))

nh_word_count <- nh_word_count %>%
  mutate(state = rep("New Hampshire"))

nj_word_count <- nj_word_count %>%
  mutate(state = rep("New Jersey"))

ny_word_count <- ny_word_count %>%
  mutate(state = rep("New York"))

pa_word_count <- pa_word_count %>%
  mutate(state = rep("Pennsylvania"))

ri_word_count <- ri_word_count %>%
  mutate(state = rep("Rhode Island"))

sc_word_count <- sc_word_count %>%
  mutate(state = rep("South Carolina"))

va_word_count <- va_word_count %>%
  mutate(state = rep("Virginia"))
```


```{r, echo=FALSE}
thirteen_cons <- rbind(ct_word_count,
                       de_word_count,
                       ga_word_count,
                       ma_word_count,
                       md_word_count,
                       nc_word_count,
                       nh_word_count,
                       nj_word_count,
                       ny_word_count,
                       pa_word_count,
                       ri_word_count,
                       sc_word_count,
                       va_word_count)
```

```{r, results='hide', message=FALSE}
states <- states()
```

```{r}
map_data <- thirteen_cons %>% 
  left_join(states, join_by(state == NAME)) %>%
  pivot_wider(names_from = word, values_from = n)

freedom_map <- ggplot(map_data, aes(fill = freedom)) + geom_sf(aes(geometry = geometry))
freedom_map
```

### Liberty
```{r}
liberty_map <- ggplot(map_data, aes(fill = liberty)) + geom_sf(aes(geometry = geometry))
liberty_map
```

### Welfare
```{r}
welfare_map <- ggplot(map_data, aes(fill = welfare)) + geom_sf(aes(geometry = geometry))
welfare_map
```

### Consent
```{r}
consent_map <- ggplot(map_data, aes(fill = consent)) + geom_sf(aes(geometry = geometry))
consent_map
```

### Prohibit
```{r}
prohibit_map <- ggplot(map_data, aes(fill = prohibit)) + geom_sf(aes(geometry = geometry))
prohibit_map
```

### Equality
```{r}
equality_map <- ggplot(map_data, aes(fill = equality)) + geom_sf(aes(geometry = geometry))
equality_map
```

### Rights
```{r}
rights_map <- ggplot(map_data, aes(fill = rights)) + geom_sf(aes(geometry = geometry))
rights_map
```

:::

## Most common words across all 13 states
```{r}
ct_all_words <- ct_con_tidy_nostop %>%
  count(word, sort = TRUE) %>%
  slice_max(n = 20, order_by = n) %>%
  mutate(state = rep("Connecticut"))


de_all_words <- de_con_tidy_nostop %>%
  count(word, sort = TRUE) %>%
  slice_max(n = 20, order_by = n) %>%
  mutate(state = rep("Delaware"))

ga_all_words <- ga_con_tidy_nostop %>%
  count(word, sort = TRUE) %>%
  slice_max(n = 20, order_by = n) %>%
  mutate(state = rep("Georgia"))

md_all_words <- md_con_tidy_nostop %>%
  count(word, sort = TRUE) %>%
  slice_max(n = 20, order_by = n) %>%
  mutate(state = rep("Maryland"))

ma_all_words <- ma_con_tidy_nostop %>%
  count(word, sort = TRUE) %>%
  slice_max(n = 20, order_by = n) %>%
  filter(!word%in%c("(art.", "")) %>%
  mutate(state = rep("Massachusetts"))

nh_all_words <- nh_con_tidy_nostop %>%
  count(word, sort = TRUE) %>%
  slice_max(n = 20, order_by = n) %>%
  filter(!word%in%c("(art.", " ")) %>%
  mutate(state = rep("New Hampshire"))

nj_all_words <- nj_con_tidy_nostop %>%
  count(word, sort = TRUE) %>%
  slice_max(n = 20, order_by = n) %>%
  filter(!word%in%c("(art.", " "))  %>%
  mutate(state = rep("New Jersey"))

ny_all_words <- ny_con_tidy_nostop %>%
  count(word, sort = TRUE) %>%
  slice_max(n = 20, order_by = n) %>%
  mutate(state = rep("New York"))

nc_all_words <- nc_con_tidy_nostop %>%
  count(word, sort = TRUE) %>%
  slice_max(n = 20, order_by = n) %>%
  filter(!word%in%c("(--", " ")) %>%
  mutate(state = rep("North Carolina"))

pa_all_words <- pa_con_tidy_nostop %>%
  count(word, sort = TRUE) %>%
  slice_max(n = 20, order_by = n) %>%
  filter(!word%in%c("(--", " ")) %>%
  mutate(state = rep("Pennsylvania"))

ri_all_words <- ri_con_tidy_nostop %>%
  count(word, sort = TRUE) %>%
  slice_max(n = 20, order_by = n) %>%
  filter(!word%in%c("(--", " ")) %>%
  mutate(state = rep("Rhode Island"))

sc_all_words <- sc_con_tidy_nostop %>%
  count(word, sort = TRUE) %>%
  slice_max(n = 20, order_by = n) %>%
  filter(!word%in%c("(--", " "))  %>%
  mutate(state = rep("South Carolina"))

va_all_words <- va_con_tidy_nostop %>%
  count(word, sort = TRUE) %>%
  slice_max(n = 20, order_by = n) %>%
  filter(!word%in%c("(--", " ")) %>%
  mutate(state = rep("Virginia"))
```

```{r, echo=FALSE}
thirteen_cons2 <- rbind(ct_all_words,
                       de_all_words,
                       ga_all_words,
                       ma_all_words,
                       md_all_words,
                       nc_all_words,
                       nh_all_words,
                       nj_all_words,
                       ny_all_words,
                       pa_all_words,
                       ri_all_words,
                       sc_all_words,
                       va_all_words)

thirteen_cons2_filtered <- thirteen_cons2 %>%
  group_by(state) %>%
  slice_max(n, n = 1) %>%
  ungroup()

all_words_map_data <- thirteen_cons2_filtered %>% 
  left_join(states, by = c("state" = "NAME"))

most_used_map <- ggplot(all_words_map_data, aes(fill = word)) +
  geom_sf(aes(geometry = geometry)) 

most_used_map
```

## Conclusions
- The most common words have to do with administration rather than ideology
- Ex. section, article, council, county, government, people
- Even though the map is a useful visualization, there don't seem to be geographical trends
